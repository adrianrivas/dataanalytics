{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\t{(b),(g),(f)}\t[jjj#3,bbb#0,ddd#9,ggg#8,hhh#2]\n",
      "A\t{(a),(f),(c)}\t[ccc#2,ddd#0,aaa#3,hhh#9]\n",
      "B\t{(f),(e),(a),(c)}\t[ddd#2,ggg#5,ccc#6,jjj#1]\n",
      "A\t{(a),(b)}\t[hhh#9,iii#5,eee#7,bbb#1]\n",
      "C\t{(f),(g),(d),(a)}\t[iii#6,ddd#5,eee#4,jjj#3]\n",
      "A\t{(c),(d)}\t[bbb#2,hhh#0,ccc#4,fff#1,aaa#7]\n",
      "A\t{(g),(d),(a)}\t[aaa#5,fff#8,ddd#2,iii#0,jjj#7,ccc#1]\n",
      "B\t{(b),(a)}\t[fff#3,hhh#1,ddd#2]\n",
      "E\t{(d),(e),(a),(f)}\t[eee#4,ccc#5,iii#9,fff#7,ggg#6,bbb#0]\n",
      "B\t{(d),(b),(g),(f)}\t[bbb#7,jjj#9,fff#5,iii#4,ggg#2,eee#3]\n"
     ]
    }
   ],
   "source": [
    "!head data.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-xr-x   - root root        512 2021-05-05 22:06 .ipynb_checkpoints\n",
      "-rw-r--r--   1 root root       1780 2021-05-18 15:07 data.tsv\n",
      "-rwxrwxrwx   1 root root       1313 2021-04-21 18:42 grader.py\n",
      "-rwxrwxrwx   1 root root        848 2021-05-18 15:06 question.pig\n",
      "-rw-r--r--   1 root root      10028 2021-05-18 15:08 script.ipynb\n",
      "Deleted data.tsv\n",
      "Found 4 items\n",
      "drwxr-xr-x   - root root        512 2021-05-05 22:06 .ipynb_checkpoints\n",
      "-rwxrwxrwx   1 root root       1313 2021-04-21 18:42 grader.py\n",
      "-rwxrwxrwx   1 root root        848 2021-05-18 15:06 question.pig\n",
      "-rw-r--r--   1 root root      10028 2021-05-18 15:08 script.ipynb\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls\n",
    "!hadoop fs -rm -r data.tsv\n",
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-xr-x   - root root        512 2021-05-05 22:06 .ipynb_checkpoints\n",
      "-rw-r--r--   1 root root       1780 2021-05-18 15:10 data.tsv\n",
      "-rwxrwxrwx   1 root root       1313 2021-04-21 18:42 grader.py\n",
      "-rwxrwxrwx   1 root root        848 2021-05-18 15:06 question.pig\n",
      "-rw-r--r--   1 root root      17150 2021-05-18 15:10 script.ipynb\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 16:04:57,628 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2021-05-18 16:04:59,646 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "2021-05-18 16:04:59,648 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "2021-05-18 16:04:59,700 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2021-05-18 16:04:59,705 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2021-05-18 16:04:59,736 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2021-05-18 16:04:59,749 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2021-05-18 16:05:00,007 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2021-05-18 16:05:00,025 [JobControl] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:00,069 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2021-05-18 16:05:00,182 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2021-05-18 16:05:00,350 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2021-05-18 16:05:00,409 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2021-05-18 16:05:00,633 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local208536610_0001\n",
      "2021-05-18 16:05:01,142 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1621353900789/pig-0.17.0-core-h2.jar <- /datalake/q05-10/pig-0.17.0-core-h2.jar\n",
      "2021-05-18 16:05:01,175 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1979288001/tmp-2088110899/pig-0.17.0-core-h2.jar as file:/tmp/hadoop-root/mapred/local/1621353900789/pig-0.17.0-core-h2.jar\n",
      "2021-05-18 16:05:01,176 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1621353900790/automaton-1.11-8.jar <- /datalake/q05-10/automaton-1.11-8.jar\n",
      "2021-05-18 16:05:01,193 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1979288001/tmp-629668079/automaton-1.11-8.jar as file:/tmp/hadoop-root/mapred/local/1621353900790/automaton-1.11-8.jar\n",
      "2021-05-18 16:05:01,195 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1621353900791/antlr-runtime-3.4.jar <- /datalake/q05-10/antlr-runtime-3.4.jar\n",
      "2021-05-18 16:05:01,201 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1979288001/tmp-674132888/antlr-runtime-3.4.jar as file:/tmp/hadoop-root/mapred/local/1621353900791/antlr-runtime-3.4.jar\n",
      "2021-05-18 16:05:01,201 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Creating symlink: /tmp/hadoop-root/mapred/local/1621353900792/joda-time-2.9.3.jar <- /datalake/q05-10/joda-time-2.9.3.jar\n",
      "2021-05-18 16:05:01,206 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - Localized file:/tmp/temp-1979288001/tmp-510683671/joda-time-2.9.3.jar as file:/tmp/hadoop-root/mapred/local/1621353900792/joda-time-2.9.3.jar\n",
      "2021-05-18 16:05:01,291 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1621353900789/pig-0.17.0-core-h2.jar\n",
      "2021-05-18 16:05:01,292 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1621353900790/automaton-1.11-8.jar\n",
      "2021-05-18 16:05:01,292 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1621353900791/antlr-runtime-3.4.jar\n",
      "2021-05-18 16:05:01,292 [JobControl] INFO  org.apache.hadoop.mapred.LocalDistributedCacheManager - file:/tmp/hadoop-root/mapred/local/1621353900792/joda-time-2.9.3.jar\n",
      "2021-05-18 16:05:01,304 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2021-05-18 16:05:01,322 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2021-05-18 16:05:01,392 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2021-05-18 16:05:01,392 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2021-05-18 16:05:01,397 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2021-05-18 16:05:01,397 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2021-05-18 16:05:01,398 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2021-05-18 16:05:01,468 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2021-05-18 16:05:01,469 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local208536610_0001_m_000000_0\n",
      "2021-05-18 16:05:01,584 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2021-05-18 16:05:01,585 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2021-05-18 16:05:01,616 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2021-05-18 16:05:01,630 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 1780\n",
      "Input split[0]:\n",
      "   Length = 1780\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2021-05-18 16:05:01,694 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2021-05-18 16:05:01,695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2021-05-18 16:05:01,695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2021-05-18 16:05:01,695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2021-05-18 16:05:01,695 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2021-05-18 16:05:01,706 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2021-05-18 16:05:01,907 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2021-05-18 16:05:01,907 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2021-05-18 16:05:01,907 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2021-05-18 16:05:01,907 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 1017; bufvoid = 104857600\n",
      "2021-05-18 16:05:01,908 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26213948(104855792); length = 449/6553600\n",
      "2021-05-18 16:05:01,998 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2021-05-18 16:05:02,004 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local208536610_0001_m_000000_0 is done. And is in the process of committing\n",
      "2021-05-18 16:05:02,019 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2021-05-18 16:05:02,019 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local208536610_0001_m_000000_0' done.\n",
      "2021-05-18 16:05:02,025 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local208536610_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5803019\n",
      "\t\tFILE: Number of bytes written=12095929\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=40\n",
      "\t\tMap output records=113\n",
      "\t\tMap output bytes=1017\n",
      "\t\tMap output materialized bytes=90\n",
      "\t\tInput split bytes=357\n",
      "\t\tCombine input records=113\n",
      "\t\tCombine output records=7\n",
      "\t\tSpilled Records=7\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=322961408\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2021-05-18 16:05:02,026 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local208536610_0001_m_000000_0\n",
      "2021-05-18 16:05:02,026 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2021-05-18 16:05:02,032 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2021-05-18 16:05:02,033 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local208536610_0001_r_000000_0\n",
      "2021-05-18 16:05:02,099 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2021-05-18 16:05:02,100 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2021-05-18 16:05:02,104 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2021-05-18 16:05:02,112 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@117a32b5\n",
      "2021-05-18 16:05:02,153 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=652528832, maxSingleShuffleLimit=163132208, mergeThreshold=430669056, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2021-05-18 16:05:02,177 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local208536610_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2021-05-18 16:05:02,287 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local208536610_0001_m_000000_0 decomp: 86 len: 90 to MEMORY\n",
      "2021-05-18 16:05:02,294 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 86 bytes from map-output for attempt_local208536610_0001_m_000000_0\n",
      "2021-05-18 16:05:02,300 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 86, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->86\n",
      "2021-05-18 16:05:02,308 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2021-05-18 16:05:02,309 [Readahead Thread #0] WARN  org.apache.hadoop.io.ReadaheadPool - Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2021-05-18 16:05:02,310 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2021-05-18 16:05:02,324 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2021-05-18 16:05:02,392 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2021-05-18 16:05:02,392 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 80 bytes\n",
      "2021-05-18 16:05:02,395 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 86 bytes to disk to satisfy reduce memory limit\n",
      "2021-05-18 16:05:02,395 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 90 bytes from disk\n",
      "2021-05-18 16:05:02,396 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2021-05-18 16:05:02,397 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2021-05-18 16:05:02,399 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 80 bytes\n",
      "2021-05-18 16:05:02,400 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2021-05-18 16:05:02,417 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1\n",
      "2021-05-18 16:05:02,417 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2021-05-18 16:05:02,425 [pool-4-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2021-05-18 16:05:02,450 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local208536610_0001_r_000000_0 is done. And is in the process of committing\n",
      "2021-05-18 16:05:02,467 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2021-05-18 16:05:02,467 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local208536610_0001_r_000000_0 is allowed to commit now\n",
      "2021-05-18 16:05:02,476 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local208536610_0001_r_000000_0' to file:/tmp/temp-1979288001/tmp-1033806082/_temporary/0/task_local208536610_0001_r_000000\n",
      "2021-05-18 16:05:02,478 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2021-05-18 16:05:02,478 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local208536610_0001_r_000000_0' done.\n",
      "2021-05-18 16:05:02,479 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local208536610_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5803231\n",
      "\t\tFILE: Number of bytes written=12096101\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=90\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=7\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=322961408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2021-05-18 16:05:02,480 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local208536610_0001_r_000000_0\n",
      "2021-05-18 16:05:02,481 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2021-05-18 16:05:06,362 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,388 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,389 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2021-05-18 16:05:06,393 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,461 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,463 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,464 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics - Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "2021-05-18 16:05:06,495 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(a,22)\n",
      "(b,12)\n",
      "(c,17)\n",
      "(d,13)\n",
      "(e,15)\n",
      "(f,25)\n",
      "(g,9)\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run question.pig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FOREACH data GENERATE minuscula;\n",
      "2021-05-18 15:40:11,979 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing. Encountered \" <IDENTIFIER> \"FOREACH \"\" at line 1, column 1.\n",
      "Was expecting one of:\n",
      "    <EOF> \n",
      "    \"cat\" ...\n",
      "    \"clear\" ...\n",
      "    \"fs\" ...\n",
      "    \"sh\" ...\n",
      "    \"cd\" ...\n",
      "    \"cp\" ...\n",
      "    \"copyFromLocal\" ...\n",
      "    \"copyToLocal\" ...\n",
      "    \"dump\" ...\n",
      "    \"\\\\d\" ...\n",
      "    \"describe\" ...\n",
      "    \"\\\\de\" ...\n",
      "    \"aliases\" ...\n",
      "    \"explain\" ...\n",
      "    \"\\\\e\" ...\n",
      "    \"help\" ...\n",
      "    \"history\" ...\n",
      "    \"kill\" ...\n",
      "    \"ls\" ...\n",
      "    \"mv\" ...\n",
      "    \"mkdir\" ...\n",
      "    \"pwd\" ...\n",
      "    \"quit\" ...\n",
      "    \"\\\\q\" ...\n",
      "    \"register\" ...\n",
      "    \"rm\" ...\n",
      "    \"rmf\" ...\n",
      "    \"set\" ...\n",
      "    \"illustrate\" ...\n",
      "    \"\\\\i\" ...\n",
      "    \"run\" ...\n",
      "    \"exec\" ...\n",
      "    \"%default\" ...\n",
      "    \"%declare\" ...\n",
      "    \"scriptDone\" ...\n",
      "    \"\" ...\n",
      "    \"\" ...\n",
      "    <EOL> ...\n",
      "    \";\" ...\n",
      "    \n",
      "Details at logfile: /datalake/q05-10/pig_1621352292913.log\n"
     ]
    }
   ],
   "source": [
    "%%pig\n",
    "FOREACH data GENERATE minuscula;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup        244 2021-05-03 16:34 data.tsv\n",
      "drwxr-xr-x   - root supergroup          0 2021-05-03 16:37 output\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,74,59:21.7\n",
      "11,74,59:22.5\n",
      "14,25,59:21.4\n",
      "18,16,59:21.7\n",
      "20,41,59:22.5\n",
      "22,87,59:21.7\n",
      "22,87,59:22.3\n",
      "23,68,59:22.4\n",
      "27,105,59:21.7\n",
      "32,42,59:22.5\n"
     ]
    }
   ],
   "source": [
    "!cat output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm output/*\n",
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
